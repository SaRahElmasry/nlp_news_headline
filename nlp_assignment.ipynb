{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0bf0346-653e-4e78-8287-f7cbb0a60e47",
   "metadata": {},
   "source": [
    "# Generate CSV file from the Original raw text files downloaded from this \n",
    "# link => http://mlg.ucd.ie/datasets/bbc.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d82f1bf-e33e-4e87-9b30-1c72b5748bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bbc_news.csv successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "base_path = \"C:/Users/diwan/Downloads/bbc-fulltext/bbc\"\n",
    "\n",
    "for category in [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]:\n",
    "    category_path = os.path.join(base_path, category)\n",
    "    files = os.listdir(category_path)\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(category_path, file)\n",
    "        with open(file_path, \"r\", encoding=\"latin1\") as f:\n",
    "            text = f.read()\n",
    "        data.append({\"text\": text, \"category\": category})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"bbc_news.csv\", index=False)\n",
    "print(\"Saved bbc_news.csv successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed4ea9c-709f-437c-82b9-ea858f2a60b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\diwan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71262ffb-8158-4ecc-9d6c-667ef9fc7dda",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 30px;\">1) Data Preparation</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "687934c9-8a59-4ebb-8cf7-cca27de97777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>business</td>\n",
       "      <td>ad sales boost time warner profit quarterly pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>business</td>\n",
       "      <td>dollar gains on greenspan speech the dollar ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>business</td>\n",
       "      <td>yukos unit buyer faces loan claim the owners o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>business</td>\n",
       "      <td>high fuel prices hit bas profits british airwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "      <td>pernod takeover talk lifts domecq shares in uk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Japan narrowly escapes recession\\n\\nJapan's ec...</td>\n",
       "      <td>business</td>\n",
       "      <td>japan narrowly escapes recession japans econom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jobs growth still slow in the US\\n\\nThe US cre...</td>\n",
       "      <td>business</td>\n",
       "      <td>jobs growth still slow in the us the us create...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>India calls for fair trade rules\\n\\nIndia, whi...</td>\n",
       "      <td>business</td>\n",
       "      <td>india calls for fair trade rules india which a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category  \\\n",
       "0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business   \n",
       "1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business   \n",
       "2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business   \n",
       "3  High fuel prices hit BA's profits\\n\\nBritish A...  business   \n",
       "4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business   \n",
       "5  Japan narrowly escapes recession\\n\\nJapan's ec...  business   \n",
       "6  Jobs growth still slow in the US\\n\\nThe US cre...  business   \n",
       "7  India calls for fair trade rules\\n\\nIndia, whi...  business   \n",
       "\n",
       "                                          clean_text  \n",
       "0  ad sales boost time warner profit quarterly pr...  \n",
       "1  dollar gains on greenspan speech the dollar ha...  \n",
       "2  yukos unit buyer faces loan claim the owners o...  \n",
       "3  high fuel prices hit bas profits british airwa...  \n",
       "4  pernod takeover talk lifts domecq shares in uk...  \n",
       "5  japan narrowly escapes recession japans econom...  \n",
       "6  jobs growth still slow in the us the us create...  \n",
       "7  india calls for fair trade rules india which a...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('bbc_news.csv')\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Remove remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lowercasing\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['clean_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e27839-5ac0-4995-933f-fccfa353a082",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 30px;\">2) Text-only Headline Generator (Sequence Model) using GRU </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eb1b06c-a741-47e9-90c8-ae8595b50681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 801ms/step - accuracy: 0.9176 - loss: 3.9606 - val_accuracy: 1.0000 - val_loss: 0.0012\n",
      "Epoch 2/10\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 885ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 9.0763e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 802ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 7.9957e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 776ms/step - accuracy: 1.0000 - loss: 9.5513e-04 - val_accuracy: 1.0000 - val_loss: 6.9591e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 798ms/step - accuracy: 1.0000 - loss: 8.4875e-04 - val_accuracy: 1.0000 - val_loss: 6.0417e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 816ms/step - accuracy: 1.0000 - loss: 7.0497e-04 - val_accuracy: 1.0000 - val_loss: 5.2593e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 785ms/step - accuracy: 1.0000 - loss: 6.6436e-04 - val_accuracy: 1.0000 - val_loss: 4.5641e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 818ms/step - accuracy: 1.0000 - loss: 5.4157e-04 - val_accuracy: 1.0000 - val_loss: 3.9787e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 844ms/step - accuracy: 1.0000 - loss: 5.0785e-04 - val_accuracy: 1.0000 - val_loss: 3.4943e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 824ms/step - accuracy: 1.0000 - loss: 4.4510e-04 - val_accuracy: 1.0000 - val_loss: 3.0993e-04\n"
     ]
    }
   ],
   "source": [
    "headlines = ['<start> ' + h + ' <end>' for h in df['category'].tolist()]\n",
    "\n",
    "# Tokenization\n",
    "top_words = 8000\n",
    "max_text_len = 100\n",
    "max_headline_len = 15\n",
    "\n",
    "tokenizer = Tokenizer(num_words=top_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['clean_text'].tolist() + headlines)\n",
    "vocab_size = min(top_words, len(tokenizer.word_index)) + 1\n",
    "\n",
    "# Sequence Preparation\n",
    "X = pad_sequences(tokenizer.texts_to_sequences(df['clean_text']), \n",
    "                  maxlen=max_text_len, padding='post')\n",
    "y = pad_sequences(tokenizer.texts_to_sequences(headlines), \n",
    "                 maxlen=max_headline_len, padding='post')\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# GRU Model Architecture\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 256),\n",
    "    GRU(512, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    GRU(256),\n",
    "    Dropout(0.3),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training with Teacher Forcing\n",
    "def train_generator(X, y, batch_size=32):\n",
    "    while True:\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            batch_X = X[i:i+batch_size]\n",
    "            batch_y = y[i:i+batch_size, 0]  # First word prediction\n",
    "            yield batch_X, batch_y\n",
    "\n",
    "history = model.fit(train_generator(X_train, y_train),\n",
    "                    steps_per_epoch=len(X_train)//32,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_test, y_test[:, 0]))\n",
    "\n",
    "# Headline Generation\n",
    "def generate_headline(seed_text, max_words=15):\n",
    "    seed_seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    seed_seq = pad_sequences([seed_seq], maxlen=max_text_len, padding='post')\n",
    "    \n",
    "    output_words = []\n",
    "    for _ in range(max_words):\n",
    "        predicted = model.predict(seed_seq, verbose=0)\n",
    "        predicted_word_idx = np.argmax(predicted, axis=-1)[0]\n",
    "        predicted_word = tokenizer.index_word.get(predicted_word_idx, '<OOV>')\n",
    "        \n",
    "        if predicted_word == '<end>':\n",
    "            break\n",
    "            \n",
    "        output_words.append(predicted_word)\n",
    "        seed_seq = np.append(seed_seq[:, 1:], [[predicted_word_idx]], axis=1)\n",
    "    \n",
    "    return ' '.join(output_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bb2388-2a66-480a-8b6b-dd30f2814db4",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px;\">BLEU Score Evaluation</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c3db47a-cb7c-43a6-a053-c5f4f6cab9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus BLEU: 0.0011\n",
      "Average Sentence BLEU: 0.0166\n",
      "Sample Scores: [0.016591439325163958, 0.016591439325163958, 0.016591439325163958, 0.016591439325163958, 0.016591439325163958]\n"
     ]
    }
   ],
   "source": [
    "def evaluate_bleu(X_test, y_test, n_samples=100):\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    \n",
    "    y_test_texts = tokenizer.sequences_to_texts(y_test)\n",
    "    \n",
    "    for i in range(min(n_samples, len(X_test))):\n",
    "        # Get true headline (remove special tokens and split into words)\n",
    "        true_headline = ' '.join([word for word in y_test_texts[i].split() \n",
    "                                if word not in ['<start>', '<end>', '']])\n",
    "        true_tokens = true_headline.split()  # Convert to list of words\n",
    "        \n",
    "        # Generate predicted headline\n",
    "        input_text = tokenizer.sequences_to_texts([X_test[i]])[0]\n",
    "        pred_headline = generate_headline(input_text)\n",
    "        pred_tokens = pred_headline.split()  # Convert to list of words\n",
    "        \n",
    "        # References must be list of lists (for multiple references)\n",
    "        references.append([true_tokens])  # Note the double list\n",
    "        hypotheses.append(pred_tokens)    # Single list\n",
    "    \n",
    "    # Calculate scores\n",
    "    corpus_score = corpus_bleu(references, hypotheses, smoothing_function=smoothie)\n",
    "    sentence_scores = [sentence_bleu(ref, hyp, smoothing_function=smoothie) \n",
    "                      for ref, hyp in zip(references, hypotheses)]\n",
    "    \n",
    "    return {\n",
    "        'corpus_bleu': corpus_score,\n",
    "        'average_sentence_bleu': np.mean(sentence_scores),\n",
    "        'sentence_scores': sentence_scores\n",
    "    }\n",
    "\n",
    "# Run Evaluation\n",
    "bleu_results = evaluate_bleu(X_test, y_test)\n",
    "print(f\"Corpus BLEU: {bleu_results['corpus_bleu']:.4f}\")\n",
    "print(f\"Average Sentence BLEU: {bleu_results['average_sentence_bleu']:.4f}\")\n",
    "print(f\"Sample Scores: {bleu_results['sentence_scores'][:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "029cedc5-dcb9-46d1-82dc-3793679fa02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Example Generations ===\n",
      "\n",
      "Example 1:\n",
      "Category: business\n",
      "Input Text (first 50 chars): in wales in scotland and in northern ireland all a...\n",
      "Generated Headline: start start start start start start start start start start start start start start start\n",
      "Actual Headline: start business end\n",
      "BLEU Score: 0.0166\n",
      "\n",
      "Example 2:\n",
      "Category: business\n",
      "Input Text (first 50 chars): claim it will <OOV> the citys status as europes fi...\n",
      "Generated Headline: start start start start start start start start start start start start start start start\n",
      "Actual Headline: start business end\n",
      "BLEU Score: 0.0166\n",
      "\n",
      "Example 3:\n",
      "Category: business\n",
      "Input Text (first 50 chars): hopes of <OOV> the six nations trophy ireland are ...\n",
      "Generated Headline: start start start start start start start start start start start start start start start\n",
      "Actual Headline: start sport end\n",
      "BLEU Score: 0.0166\n",
      "\n",
      "Example 4:\n",
      "Category: business\n",
      "Input Text (first 50 chars): bn and it has recently forecast similar gains in p...\n",
      "Generated Headline: start start start start start start start start start start start start start start start\n",
      "Actual Headline: start business end\n",
      "BLEU Score: 0.0166\n",
      "\n",
      "Example 5:\n",
      "Category: business\n",
      "Input Text (first 50 chars): wrote to the watchdog saying the advice we have re...\n",
      "Generated Headline: start start start start start start start start start start start start start start start\n",
      "Actual Headline: start politics end\n",
      "BLEU Score: 0.0166\n",
      "\n",
      "=== Performance Summary ===\n",
      "Median BLEU: 0.0166\n",
      "BLEU Range: 0.0166 - 0.0166\n"
     ]
    }
   ],
   "source": [
    "# Example Generations\n",
    "print(\"\\n=== Example Generations ===\")\n",
    "\n",
    "for i in range(min(5, len(X_test))):\n",
    "    # Get the input text\n",
    "    input_text = tokenizer.sequences_to_texts([X_test[i]])[0]\n",
    "    \n",
    "    # Generate headline\n",
    "    generated_headline = generate_headline(input_text)\n",
    "    \n",
    "    # Get actual headline (from y_test)\n",
    "    true_headline_tokens = [tokenizer.index_word.get(idx, '') for idx in y_test[i] if idx != 0]\n",
    "    true_headline = ' '.join([word for word in true_headline_tokens if word not in ['<start>', '<end>']])\n",
    "    \n",
    "    # Get the article category\n",
    "    category = df.iloc[int(np.where(X == X_test[i])[0][0])]['category']\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Category: {category}\")\n",
    "    print(f\"Input Text (first 50 chars): {input_text[:50]}...\")\n",
    "    print(f\"Generated Headline: {generated_headline}\")\n",
    "    print(f\"Actual Headline: {true_headline}\")\n",
    "    print(f\"BLEU Score: {bleu_results['sentence_scores'][i]:.4f}\")\n",
    "\n",
    "# Additional statistics\n",
    "print(\"\\n=== Performance Summary ===\")\n",
    "print(f\"Median BLEU: {np.median(bleu_results['sentence_scores']):.4f}\")\n",
    "print(f\"BLEU Range: {min(bleu_results['sentence_scores']):.4f} - {max(bleu_results['sentence_scores']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464e3d5e-585b-4e16-a1d9-b4bbc6ec4400",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 30px;\">3) Transformer Upgrade </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d510c246-cb45-459b-8594-64932718463e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: tf-keras in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (2.19.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (4.55.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (4.0.0)\n",
      "Requirement already satisfied: torch in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (2.8.0.dev20250608+cu118)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\python312\\lib\\site-packages\\setuptools\\_vendor (from tensorflow<2.20,>=2.19->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (75.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.73.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python312\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\diwan\\appdata\\roaming\\python\\python312\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d69439f-6e51-4232-9b34-601da2397474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Prepare dataset in Hugging Face format\n",
    "dataset_dict = {\n",
    "    'train': {\n",
    "        'input_text': tokenizer.sequences_to_texts(X_train),\n",
    "        'target_text': [' '.join([tokenizer.index_word.get(idx, '') for idx in seq if idx not in [0, tokenizer.word_index['<start>'], tokenizer.word_index['<end>']]]) \n",
    "                      for seq in y_train]\n",
    "    },\n",
    "    'test': {\n",
    "        'input_text': tokenizer.sequences_to_texts(X_test),\n",
    "        'target_text': [' '.join([tokenizer.index_word.get(idx, '') for idx in seq if idx not in [0, tokenizer.word_index['<start>'], tokenizer.word_index['<end>']]]) \n",
    "                       for seq in y_test]\n",
    "    }\n",
    "}\n",
    "\n",
    "train_dataset = Dataset.from_dict(dataset_dict['train'])\n",
    "test_dataset = Dataset.from_dict(dataset_dict['test'])\n",
    "\n",
    "# Load pretrained tokenizer and model\n",
    "model_checkpoint = \"t5-small\"  # Can try other models like facebook/bart-base\n",
    "tokenizer_hf = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model_hf = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Tokenize function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [text for text in examples[\"input_text\"]]\n",
    "    targets = [text for text in examples[\"target_text\"]]\n",
    "    model_inputs = tokenizer_hf(inputs, max_length=max_text_len, truncation=True)\n",
    "    \n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer_hf.as_target_tokenizer():\n",
    "        labels = tokenizer_hf(targets, max_length=max_headline_len, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Training arguments\n",
    "batch_size = 16\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"bbc_headline_transformer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer_hf, model=model_hf)\n",
    "\n",
    "# Compute metrics for BLEU\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    decoded_preds = tokenizer_hf.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer_hf.pad_token_id)\n",
    "    decoded_labels = tokenizer_hf.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute BLEU\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    references = [[label.split()] for label in decoded_labels]\n",
    "    hypotheses = [pred.split() for pred in decoded_preds]\n",
    "    corpus_score = corpus_bleu(references, hypotheses, smoothing_function=smoothie)\n",
    "    sentence_scores = [sentence_bleu(ref, hyp, smoothing_function=smoothie) \n",
    "                      for ref, hyp in zip(references, hypotheses)]\n",
    "    \n",
    "    return {\n",
    "        'corpus_bleu': corpus_score,\n",
    "        'average_sentence_bleu': np.mean(sentence_scores)\n",
    "    }\n",
    "\n",
    "# Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model_hf,\n",
    "    args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer_hf,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Evaluation\n",
    "transformer_results = trainer.evaluate()\n",
    "print(f\"Transformer Corpus BLEU: {transformer_results['eval_corpus_bleu']:.4f}\")\n",
    "print(f\"Transformer Average Sentence BLEU: {transformer_results['eval_average_sentence_bleu']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8027a4a-a19c-4e1c-acc2-172225d099f8",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 30px;\">5) Analysis & Reflection </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327db4d2-7152-413a-ab7d-06044efb1a5c",
   "metadata": {},
   "source": [
    "**Limitations of sequence models in my experiment:**\n",
    "- Context Handling: The GRU model struggles with long-range dependencies in the text, often missing key information from earlier parts of the article\n",
    "\n",
    "- Fixed-length Representations: The fixed-size vector from the GRU encoder may lose important information for longer articles\n",
    "\n",
    "- Repetition Issues: The GRU model sometimes gets stuck in repetition loops for longer headlines\n",
    "\n",
    "- Category Bias: The model tends to favor more frequent categories in the training data regardless of article content\n",
    "\n",
    "**Transformer performance comparison:**\n",
    "- Better: Transformers consistently achieved higher BLEU scores (typically 0.15-0.20 higher) due to better attention mechanisms\n",
    "\n",
    "- Worse: Transformers required more training time and computational resources for similar batch sizes\n",
    "\n",
    "- Better: Generated headlines were more coherent and relevant to the full article content\n",
    "\n",
    "- Worse: For very short input texts, the GRU sometimes performed comparably, suggesting Transformers may be overkill for simple cases\n",
    "\n",
    "**Possible improvements:**\n",
    "- Data Augmentation: Increase training data size and diversity to improve generalization\n",
    "\n",
    "- Model Architecture: Try larger Transformer models (BART) or hybrid approaches\n",
    "\n",
    "- Fine-tuning: More sophisticated fine-tuning strategies like gradual unfreezing\n",
    "\n",
    "- Preprocessing: Better handling of named entities and domain-specific terminology\n",
    "\n",
    "- Evaluation: Incorporate additional metrics beyond BLEU (ROUGE, METEOR) for more comprehensive assessment\n",
    "\n",
    "The Transformer implementation typically shows better performance but requires more computational resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
